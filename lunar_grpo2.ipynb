{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1ec2a3f-fc12-4287-ba43-6629a6a93a07",
   "metadata": {},
   "source": [
    "# GRPO from Scratch\n",
    "\n",
    "> Grouped Relative Policy Optimization\n",
    "\n",
    "GRPO is in fact just a simpler version of PPO without the reward function. So most "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7b79cd2-18ce-45a9-8649-c7be189c9fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from torch import nn, optim, Tensor\n",
    "import einx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "device = t.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be3571cc-87b0-4c3a-b946-3a7fa3c5927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EnvConfig:\n",
    "    envs: int = 10\n",
    "    steps: int = 128\n",
    "    generations: int = 4\n",
    "\n",
    "    @property\n",
    "    def tot_envs(self): return self.envs * self.generations\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    episodes: int = 10\n",
    "    lr: float = 1e-3\n",
    "    gamma: float = 0.999 # discount factor\n",
    "    ent_coef = 1e-2 # entropy bonus (encourage exploration)\n",
    "\n",
    "env_cfg = EnvConfig()\n",
    "trn_cfg = TrainConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1226dfc3-f976-4081-b71f-7eb8f04f8b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoopResetWrapper(gym.vector.VectorWrapper):\n",
    "    def step(\n",
    "        self, actions: ActType,\n",
    "    ) -> tuple[ObsType, ArrayType, ArrayType, ArrayType, dict[str, Any]]:\n",
    "        \"\"\"Steps through each of the environments returning the batched results.\n",
    "\n",
    "        Returns:\n",
    "            The batched environment step results\n",
    "        \"\"\"\n",
    "        actions = iterate(self.action_space, actions)\n",
    "\n",
    "        infos = {}\n",
    "        for i, action in enumerate(actions):\n",
    "            if self.autoreset_mode == AutoresetMode.NEXT_STEP:\n",
    "                if self._autoreset_envs[i]:\n",
    "                    self._env_obs[i], env_info = self.envs[i].reset()\n",
    "\n",
    "                    self._rewards[i] = 0.0\n",
    "                    self._terminations[i] = False\n",
    "                    self._truncations[i] = False\n",
    "                else:\n",
    "                    (\n",
    "                        self._env_obs[i],\n",
    "                        self._rewards[i],\n",
    "                        self._terminations[i],\n",
    "                        self._truncations[i],\n",
    "                        env_info,\n",
    "                    ) = self.envs[i].step(action)\n",
    "            elif self.autoreset_mode == AutoresetMode.DISABLED:\n",
    "                if self._autoreset_envs[i]: continue\n",
    "                    \n",
    "                (\n",
    "                    self._env_obs[i],\n",
    "                    self._rewards[i],\n",
    "                    self._terminations[i],\n",
    "                    self._truncations[i],\n",
    "                    env_info,\n",
    "                ) = self.envs[i].step(action)\n",
    "            elif self.autoreset_mode == AutoresetMode.SAME_STEP:\n",
    "                (\n",
    "                    self._env_obs[i],\n",
    "                    self._rewards[i],\n",
    "                    self._terminations[i],\n",
    "                    self._truncations[i],\n",
    "                    env_info,\n",
    "                ) = self.envs[i].step(action)\n",
    "\n",
    "                if self._terminations[i] or self._truncations[i]:\n",
    "                    infos = self._add_info(\n",
    "                        infos,\n",
    "                        {\"final_obs\": self._env_obs[i], \"final_info\": env_info},\n",
    "                        i,\n",
    "                    )\n",
    "\n",
    "                    self._env_obs[i], env_info = self.envs[i].reset()\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected autoreset mode, {self.autoreset_mode}\")\n",
    "\n",
    "            infos = self._add_info(infos, env_info, i)\n",
    "\n",
    "        # Concatenate the observations\n",
    "        self._observations = concatenate(\n",
    "            self.single_observation_space, self._env_obs, self._observations\n",
    "        )\n",
    "        self._autoreset_envs = np.logical_or(self._terminations, self._truncations)\n",
    "\n",
    "        return (\n",
    "            deepcopy(self._observations) if self.copy else self._observations,\n",
    "            np.copy(self._rewards),\n",
    "            np.copy(self._terminations),\n",
    "            np.copy(self._truncations),\n",
    "            infos,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "516928d5-89dd-4eb1-819e-f3a24b75345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_envs = gym.make_vec(\n",
    "    \"LunarLander-v3\",\n",
    "    num_envs=env_cfg.tot_envs,\n",
    "    max_episode_steps=env_cfg.steps, \n",
    "    vectorization_mode=\"sync\",\n",
    "    vector_kwargs=dict(autoreset_mode=gym.vector.AutoresetMode.DISABLED),\n",
    ")\n",
    "envs = gym.wrappers.vector.RecordEpisodeStatistics(\n",
    "    _envs,\n",
    "    buffer_length=env_cfg.tot_envs * env_cfg.steps\n",
    ")\n",
    "\n",
    "envs = NoopResetWrapper(envs)\n",
    "\n",
    "obs_shape = envs.single_observation_space.shape[0]\n",
    "action_shape = envs.single_action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "61e70c82-e898-4d28-8327-28ffa01490a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(rewards, log_probs):\n",
    "    reward_per_generation = einx.sum(\"(env gen) [step] -> env gen\", rewards, gen=env_cfg.generations)\n",
    "    mean_reward = einx.mean(\"env [gen]\", reward_per_generation)\n",
    "    advantages = einx.subtract(\n",
    "        \"env gen, env -> env gen\",\n",
    "        reward_per_generation,\n",
    "        mean_reward,\n",
    "    )\n",
    "    \n",
    "    # std_rewards = einx.std(\"n_envs [n_generations]\", reward_per_generation) + 1e-8\n",
    "    # advantages = einx.divide(\"[n_envs n_generations], [n_envs]\", advantages, std_rewards)\n",
    "    \n",
    "    discounts = torch.tensor([gamma ** i for i in range(n_steps_per_update)][::-1], device=device)\n",
    "    per_step_advantages = einx.multiply(\n",
    "        \"env gen, step -> env gen step\",\n",
    "        advantages,\n",
    "        discounts,\n",
    "    )\n",
    "    \n",
    "    # ratio = torch.exp(action_log_probs - action_log_probs)\n",
    "    pg_loss1 = -einx.multiply(\"env gen step, (env gen) step -> env gen step\", per_step_advantages, log_probs).mean()\n",
    "    return pg_loss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5152c84d-2241-4263-bb31-2a59571a833c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'iterate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[134]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     54\u001b[39m actions, log_probs, entropy = sample(logits)\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# get next state from actions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m states, rewards, terminated, truncated, info = \u001b[43menvs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m tt_mask = t.from_numpy(terminated | truncated).to(device)\n\u001b[32m     60\u001b[39m ep_log_probs[:, step] = log_probs\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[131]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mNoopResetWrapper.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mself\u001b[39m, actions: ActType,\n\u001b[32m      4\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[ObsType, ArrayType, ArrayType, ArrayType, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m      5\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through each of the environments returning the batched results.\u001b[39;00m\n\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03m        The batched environment step results\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     actions = \u001b[43miterate\u001b[49m(\u001b[38;5;28mself\u001b[39m.action_space, actions)\n\u001b[32m     12\u001b[39m     infos = {}\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(actions):\n",
      "\u001b[31mNameError\u001b[39m: name 'iterate' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "agent = nn.Sequential(\n",
    "    nn.Linear(obs_shape, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, action_shape),\n",
    ").to(device)\n",
    "\n",
    "opt = optim.RMSprop(agent.parameters(), lr=trn_cfg.lr)\n",
    "\n",
    "\n",
    "def sample(logits):\n",
    "    \"\"\"\n",
    "    Samples an action from logits\n",
    "\n",
    "    Args:\n",
    "        logits: (tot_envs, actions)\n",
    "\n",
    "    Returns:\n",
    "        actions: Sampled action (tot_envs,)\n",
    "        log_probs: Log-prob for sampled action (tot_envs,)\n",
    "        entropy: Entropy of distributions (tot_envs,)\n",
    "    \"\"\"\n",
    "    pd = t.distributions.Categorical(logits=logits) # softmax\n",
    "    actions = pd.sample()\n",
    "    log_probs = pd.log_prob(actions)\n",
    "    entropy = pd.entropy()\n",
    "    return actions, log_probs, entropy\n",
    "\n",
    "def maybe_reset_envs(envs, seed, tt_mask):\n",
    "    consts = einx.solve(\"(env gen)\", tt_mask, gen=env_cfg.generations)\n",
    "    \n",
    "    seeds = seed + einx.rearrange(\"env -> (env gen)\", np.arange(env_cfg.envs), **consts)\n",
    "    \n",
    "    envs_to_reset = einx.all(\"(env [gen])\", tt_mask, **consts) # all generations done\n",
    "    reset_mask = einx.rearrange(\"env -> (env gen)\", envs_to_reset, **consts)\n",
    "\n",
    "    if np.any(reset_mask):\n",
    "        states, info = envs.reset(seed=seeds.tolist(), options=dict(reset_mask=reset_mask))\n",
    "        next_seed = seeds.max() + 1\n",
    "        return states, info, next_seed.item()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# reset all envs at the start\n",
    "states, info, seed = maybe_reset_envs(envs, seed=0, tt_mask=np.ones(env_cfg.tot_envs))\n",
    "\n",
    "ep_log_probs = t.zeros((env_cfg.tot_envs, env_cfg.steps), dtype=t.float32, device=device)\n",
    "ep_rewards = t.zeros((env_cfg.tot_envs, env_cfg.steps), dtype=t.float32, device=device)\n",
    "ep_masks = t.full((env_cfg.tot_envs, env_cfg.steps), False, dtype=t.bool, device=device)\n",
    "for step in range(env_cfg.steps):\n",
    "    # sample next action\n",
    "    logits = agent(t.from_numpy(states).to(device))\n",
    "    actions, log_probs, entropy = sample(logits)\n",
    "\n",
    "    # get next state from actions\n",
    "    states, rewards, terminated, truncated, info = envs.step(actions.cpu().numpy())\n",
    "    tt_mask = t.from_numpy(terminated | truncated).to(device)\n",
    "\n",
    "    ep_log_probs[:, step] = log_probs\n",
    "    ep_rewards[:, step] = t.from_numpy(rewards).float().to(device)\n",
    "    ep_masks[:, step] = tt_mask\n",
    "\n",
    "    res = maybe_reset_envs(envs, seed, tt_mask.cpu().numpy())\n",
    "    if res is not None:\n",
    "        states, info, seed = res\n",
    "\n",
    "loss_func(ep_rewards, ep_log_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680badbd-e8d8-4233-9276-3db4b9f18b48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
