{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27b244d8-0968-44b6-93e6-000af6837154",
   "metadata": {},
   "source": [
    "# Advantage Actor-Critic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af76d11-a063-407e-8320-cd2fc50fe059",
   "metadata": {},
   "source": [
    "For each minibatch:\n",
    "- We do `n_steps_per_update` in `n_envs` environments in parallel.\n",
    "    - (This gives `n_steps_per_update * n_envs` steps in total per minibatch.)\n",
    "\n",
    "To calculate the advantages, we are using _Generalized Advantage Estimation_ (GAE).\n",
    "\n",
    "- Balances tradeoff between variance and bias of advantage estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47cdf9ab-22ee-4b73-8355-83e1de33ff74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import einx\n",
    "from torch import nn, optim, Tensor\n",
    "from tqdm.auto import tqdm, trange\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfca15f-f2ad-4728-9b64-c10d72bda10f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614b5a10-3c71-484c-ae2e-e16a46114afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoopTerminateWrapper(gym.Wrapper):\n",
    "    \"\"\"A class for providing an automatic reset functionality for gym environments when calling :meth:`self.step`.\n",
    "\n",
    "    When calling step causes :meth:`Env.step` to return `terminated=True` or `truncated=True`, :meth:`Env.reset` is called,\n",
    "    and the return format of :meth:`self.step` is as follows: ``(new_obs, final_reward, final_terminated, final_truncated, info)``\n",
    "    with new step API and ``(new_obs, final_reward, final_done, info)`` with the old step API.\n",
    "     - ``new_obs`` is the first observation after calling :meth:`self.env.reset`\n",
    "     - ``final_reward`` is the reward after calling :meth:`self.env.step`, prior to calling :meth:`self.env.reset`.\n",
    "     - ``final_terminated`` is the terminated value before calling :meth:`self.env.reset`.\n",
    "     - ``final_truncated`` is the truncated value before calling :meth:`self.env.reset`. Both `final_terminated` and `final_truncated` cannot be False.\n",
    "     - ``info`` is a dict containing all the keys from the info dict returned by the call to :meth:`self.env.reset`,\n",
    "       with an additional key \"final_observation\" containing the observation returned by the last call to :meth:`self.env.step`\n",
    "       and \"final_info\" containing the info dict returned by the last call to :meth:`self.env.step`.\n",
    "\n",
    "    Warning: When using this wrapper to collect rollouts, note that when :meth:`Env.step` returns `terminated` or `truncated`, a\n",
    "        new observation from after calling :meth:`Env.reset` is returned by :meth:`Env.step` alongside the\n",
    "        final reward, terminated and truncated state from the previous episode.\n",
    "        If you need the final state from the previous episode, you need to retrieve it via the\n",
    "        \"final_observation\" key in the info dict.\n",
    "        Make sure you know what you're doing if you use this wrapper!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: gym.Env):\n",
    "        \"\"\"A class for providing an automatic reset functionality for gym environments when calling :meth:`self.step`.\n",
    "\n",
    "        Args:\n",
    "            env (gym.Env): The environment to apply the wrapper\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "\n",
    "    def step(self, action, mask=None):\n",
    "        \"\"\"Steps through only the environments specified in the mask returning the results.\n",
    "        \n",
    "        Args:\n",
    "            actions: Actions to take in each environment\n",
    "            mask: Boolean array indicating which environments to step (True) and which to skip (False)\n",
    "        \n",
    "        Returns:\n",
    "            The batched environment step results (observations, rewards, terminations, truncations, infos)\n",
    "        \"\"\"\n",
    "        if mask is None:\n",
    "            mask = np.full_like(actions, True, dtype=np.bool)\n",
    "        \n",
    "        actions = gym.vector.utils.iterate(self.env.action_space, actions)\n",
    "        \n",
    "        # Make a copy of the current state to avoid modifying environments we're not stepping\n",
    "        masked_rewards = np.copy(self.env._rewards)\n",
    "        masked_terminations = np.copy(self.env._terminations)\n",
    "        masked_truncations = np.copy(self.env._truncations)\n",
    "        \n",
    "        infos = {}\n",
    "        for i, action in enumerate(actions):\n",
    "            # Skip environments not in mask\n",
    "            if not mask[i]:\n",
    "                continue\n",
    "                \n",
    "            if self.env.autoreset_mode == AutoresetMode.NEXT_STEP:\n",
    "                if self.env._autoreset_envs[i]:\n",
    "                    self.env._env_obs[i], env_info = self.env.envs[i].reset()\n",
    "    \n",
    "                    masked_rewards[i] = 0.0\n",
    "                    masked_terminations[i] = False\n",
    "                    masked_truncations[i] = False\n",
    "                else:\n",
    "                    (\n",
    "                        self.env._env_obs[i],\n",
    "                        masked_rewards[i],\n",
    "                        masked_terminations[i],\n",
    "                        masked_truncations[i],\n",
    "                        env_info,\n",
    "                    ) = self.env.envs[i].step(action)\n",
    "            elif self.env.autoreset_mode == AutoresetMode.DISABLED:\n",
    "                # assumes that the user has correctly autoreset\n",
    "                assert not self.env._autoreset_envs[i], f\"{self.env._autoreset_envs=}\"\n",
    "                (\n",
    "                    self.env._env_obs[i],\n",
    "                    masked_rewards[i],\n",
    "                    masked_terminations[i],\n",
    "                    masked_truncations[i],\n",
    "                    env_info,\n",
    "                ) = self.env.envs[i].step(action)\n",
    "            elif self.env.autoreset_mode == AutoresetMode.SAME_STEP:\n",
    "                (\n",
    "                    self.env._env_obs[i],\n",
    "                    masked_rewards[i],\n",
    "                    masked_terminations[i],\n",
    "                    masked_truncations[i],\n",
    "                    env_info,\n",
    "                ) = self.env.envs[i].step(action)\n",
    "    \n",
    "                if masked_terminations[i] or masked_truncations[i]:\n",
    "                    infos = self.env._add_info(\n",
    "                        infos,\n",
    "                        {\"final_obs\": self.env._env_obs[i], \"final_info\": env_info},\n",
    "                        i,\n",
    "                    )\n",
    "    \n",
    "                    self.env._env_obs[i], env_info = self.env.envs[i].reset()\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected autoreset mode, {self.env.autoreset_mode}\")\n",
    "    \n",
    "            infos = self.env._add_info(infos, env_info, i)\n",
    "    \n",
    "        # Update the internal state variables\n",
    "        self.env._rewards = masked_rewards\n",
    "        self.env._terminations = masked_terminations\n",
    "        self.env._truncations = masked_truncations\n",
    "        \n",
    "        # Only update _autoreset_envs for the masked environments\n",
    "        masked_autoreset = np.logical_or(masked_terminations, masked_truncations)\n",
    "        self.env._autoreset_envs[mask] = masked_autoreset[mask]\n",
    "    \n",
    "        # Concatenate the observations\n",
    "        self.env._observations = concatenate(\n",
    "            self.env.single_observation_space, self.env._env_obs, self.env._observations\n",
    "        )\n",
    "    \n",
    "        return (\n",
    "            deepcopy(self.env._observations) if self.env.copy else self.env._observations,\n",
    "            np.copy(self.env._rewards),\n",
    "            np.copy(self.env._terminations),\n",
    "            np.copy(self.env._truncations),\n",
    "            infos,\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "588310ff-c737-4aa1-9b18-ca5725811c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRPO(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features,\n",
    "        n_actions,\n",
    "        device,\n",
    "        actor_lr,\n",
    "        n_envs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.n_envs = n_envs\n",
    "        \n",
    "        # Advantage of an action is the difference between\n",
    "        # the return and state-value\n",
    "        # A(s, a) = Q(s, a) - V(s)\n",
    "        #\n",
    "        # **Q:** how do we know Q(s, a)? is this the discount factor thing\n",
    "        actor_layers = [\n",
    "            nn.Linear(n_features, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            # estimate action logits (will be fed into a softmax later)\n",
    "            nn.Linear(32, n_actions),\n",
    "        ]\n",
    "\n",
    "        self.actor = nn.Sequential(*actor_layers).to(self.device)\n",
    "        self.actor_optim = optim.RMSprop(self.actor.parameters(), lr=actor_lr)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x: np.ndarray):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Current states (env, feat)\n",
    "\n",
    "        Returns:\n",
    "            action_logits: Actor estimated actions (env, action)\n",
    "        \"\"\"\n",
    "        x = torch.from_numpy(x).to(self.device)\n",
    "        action_logits = self.actor(x)\n",
    "        return action_logits\n",
    "\n",
    "    def select_action(self, x):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            actions: Sampled actions to update state (n_envs,)\n",
    "            action_log_probs: Log softmax of actions (n_envs, n_actions)\n",
    "            entropy: (n_envs,)\n",
    "        \"\"\"\n",
    "        action_logits = self.forward(x)\n",
    "        # uses softmax\n",
    "        action_pd = torch.distributions.Categorical(logits=action_logits)\n",
    "        actions = action_pd.sample()\n",
    "        action_log_probs = action_pd.log_prob(actions)\n",
    "        entropy = action_pd.entropy()\n",
    "        return actions, action_log_probs, entropy\n",
    "\n",
    "    def get_losses(\n",
    "        self,\n",
    "        rewards,\n",
    "        action_log_probs,\n",
    "        entropy,\n",
    "        masks,\n",
    "        gamma: float,\n",
    "        ent_coef: float,\n",
    "        n_generations: int,\n",
    "        n_steps_per_update: int,\n",
    "        device,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Computes loss of a minibatch (transitions collected in one sampling phase)\n",
    "        for on-policy GRPO.\n",
    "\n",
    "        Uses standard action_log_probs rather than ratio like in PPO\n",
    "    \n",
    "        Args:\n",
    "            rewards: Rewards for each time step in episode (step, (env, gen))\n",
    "            action_log_probs: logprob of action taken at each time step in episode (step, (env, gen))\n",
    "            masks: Masks for each time step in episode\n",
    "            gamma: Discount factor\n",
    "            ent_coef: Entropy coefficeint\n",
    "            n_generations (int): Number of generations per env\n",
    "            n_steps_per_update (int)\n",
    "            device\n",
    "\n",
    "        Returns:\n",
    "            actor_loss\n",
    "        \"\"\"\n",
    "        rewards = einx.rearrange(\n",
    "            \"step (env gen) -> env gen step\",\n",
    "            rewards,\n",
    "            gen=n_generations,\n",
    "        )\n",
    "        reward_per_generation = einx.sum(\"env gen [step]\", rewards)\n",
    "        mean_reward = einx.mean(\"env [gen]\", reward_per_generation)\n",
    "        \n",
    "        advantages = einx.subtract(\n",
    "            \"env gen, env -> env gen\",\n",
    "            reward_per_generation,\n",
    "            mean_reward,\n",
    "        )\n",
    "        \n",
    "        # std_rewards = einx.std(\"n_envs [n_generations]\", reward_per_generation) + 1e-8\n",
    "        # advantages = einx.divide(\"[n_envs n_generations], [n_envs]\", advantages, std_rewards)\n",
    "        \n",
    "        discounts = torch.tensor([gamma ** i for i in range(n_steps_per_update)][::-1], device=device)\n",
    "        per_step_advantages = einx.multiply(\n",
    "            \"env gen, step -> env gen step\",\n",
    "            advantages,\n",
    "            discounts,\n",
    "        )\n",
    "\n",
    "        \n",
    "        # ratio = torch.exp(action_log_probs - action_log_probs)\n",
    "        pg_loss1 = -einx.multiply(\"env gen step, step (env gen) -> env gen step\", per_step_advantages, action_log_probs).mean()\n",
    "        return pg_loss1\n",
    "\n",
    "\n",
    "\n",
    "        # action_log_probs = einx.rearrange(\n",
    "        #     \"n_steps_per_update, (n_envs, n_generations) n_actions -> n_envs n_generations n_steps_per_update n_actions\",\n",
    "        #     action_log_probs,\n",
    "        #     n_generations=self.n_generations,\n",
    "        # )\n",
    "\n",
    "        # return actor loss at some point\n",
    "\n",
    "\n",
    "    def update_parameters(self, actor_loss):\n",
    "        self.actor_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80616abe-a239-4513-aa53-29aed02773f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# envs = gym.vector.VectorEnv(\n",
    "#     [\n",
    "#         lambda: gym.make(\n",
    "#             \"LunarLander-v3\",\n",
    "#             gravity=-10.0,\n",
    "#             enable_wind=True,\n",
    "#             wind_power=15.0,\n",
    "#             turbulence_power=1.5,\n",
    "#             max_episode_steps=600,\n",
    "#         ),\n",
    "#         lambda: gym.make(\n",
    "#             \"LunarLander-v3\",\n",
    "#             gravity=-9.8,\n",
    "#             enable_wind=True,\n",
    "#             wind_power=10.0,\n",
    "#             turbulence_power=1.3,\n",
    "#             max_episode_steps=600,\n",
    "#         ),\n",
    "#         lambda: gym.make(\n",
    "#             \"LunarLander-v3\", gravity=-7.0, enable_wind=False, max_episode_steps=600\n",
    "#         ),\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1536222-a99d-4402-9bd9-935ec9e92dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_envs = 10\n",
    "n_updates = 200\n",
    "n_steps_per_update = 128\n",
    "n_generations = 4\n",
    "\n",
    "total_envs = n_envs * n_generations\n",
    "\n",
    "# hyperparameters\n",
    "gamma = 0.999\n",
    "ent_coef = 0.01 # coef for entropy bonus (encourage exploration)\n",
    "actor_lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f0313ce-0622-4c2d-9947-2b81090e826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = gym.make_vec(\n",
    "    \"LunarLander-v3\",\n",
    "    num_envs=total_envs,\n",
    "    max_episode_steps=600, \n",
    "    vectorization_mode=\"sync\",\n",
    "    vector_kwargs=dict(autoreset_mode=gym.vector.AutoresetMode.DISABLED),\n",
    ")\n",
    "envs_wrapper = gym.wrappers.vector.RecordEpisodeStatistics(\n",
    "    envs,\n",
    "    buffer_length=n_envs * n_updates\n",
    ")\n",
    "\n",
    "obs_shape = envs.single_observation_space.shape[0]\n",
    "action_shape = envs.single_action_space.n\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "agent = GRPO(obs_shape, action_shape, device, actor_lr, n_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2d3c91-0bf8-4048-92c7-9e6344aed07f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4474801-cd03-4c04-9a16-28a48d418312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca2dbcc54dc48d4a6c10a1fe2e3162d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'AutoresetMode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     19\u001b[39m (\n\u001b[32m     20\u001b[39m     actions,\n\u001b[32m     21\u001b[39m     action_log_probs,\n\u001b[32m     22\u001b[39m     entropy,\n\u001b[32m     23\u001b[39m ) = agent.select_action(states)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# perform actions[t] in env to get s[t+1] and r[t+1]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m states, rewards, terminated, truncated, info = \u001b[43menvs_wrapper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m ep_action_log_probs[step] = action_log_probs\n\u001b[32m     29\u001b[39m ep_rewards[step] = torch.tensor(rewards, device=device, dtype=torch.float32)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/gym/.venv/lib/python3.12/site-packages/gymnasium/wrappers/vector/common.py:150\u001b[39m, in \u001b[36mRecordEpisodeStatistics.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    141\u001b[39m     \u001b[38;5;28mself\u001b[39m, actions: ActType\n\u001b[32m    142\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[ObsType, ArrayType, ArrayType, ArrayType, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[32m    143\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment, recording the episode statistics.\"\"\"\u001b[39;00m\n\u001b[32m    144\u001b[39m     (\n\u001b[32m    145\u001b[39m         observations,\n\u001b[32m    146\u001b[39m         rewards,\n\u001b[32m    147\u001b[39m         terminations,\n\u001b[32m    148\u001b[39m         truncations,\n\u001b[32m    149\u001b[39m         infos,\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    153\u001b[39m         infos, \u001b[38;5;28mdict\u001b[39m\n\u001b[32m    154\u001b[39m     ), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`vector.RecordEpisodeStatistics` requires `info` type to be `dict`, its actual type is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(infos)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. This may be due to usage of other wrappers in the wrong order.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    156\u001b[39m     \u001b[38;5;28mself\u001b[39m.episode_returns[\u001b[38;5;28mself\u001b[39m.prev_dones] = \u001b[32m0\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mmasked_step\u001b[39m\u001b[34m(self, actions, mask)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mask[i]:\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.autoreset_mode == \u001b[43mAutoresetMode\u001b[49m.NEXT_STEP:\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._autoreset_envs[i]:\n\u001b[32m     31\u001b[39m         \u001b[38;5;28mself\u001b[39m._env_obs[i], env_info = \u001b[38;5;28mself\u001b[39m.envs[i].reset()\n",
      "\u001b[31mNameError\u001b[39m: name 'AutoresetMode' is not defined"
     ]
    }
   ],
   "source": [
    "actor_losses = []\n",
    "entropies = []\n",
    "\n",
    "states, info = envs_wrapper.reset(seed=42)\n",
    "\n",
    "# don't have to reset envs, they keep playing until episode over and reset automatically\n",
    "# how to set n_generations envs to be the same?\n",
    "\n",
    "pbar = trange(n_updates)\n",
    "for sample_phase in pbar:\n",
    "    # reset lists that collect experience of an episode\n",
    "    ep_action_log_probs = torch.zeros(n_steps_per_update, total_envs, device=device)\n",
    "    ep_rewards = torch.zeros(n_steps_per_update, total_envs, device=device)\n",
    "    masks = torch.zeros(n_steps_per_update, total_envs, device=device)\n",
    "    reset_mask = torch.full((total_envs,), False, device=device, dtype=torch.bool)\n",
    "\n",
    "    for step in range(n_steps_per_update):\n",
    "        # sample actions a[t] using s[t] as input\n",
    "        (\n",
    "            actions,\n",
    "            action_log_probs,\n",
    "            entropy,\n",
    "        ) = agent.select_action(states)\n",
    "\n",
    "        # perform actions[t] in env to get s[t+1] and r[t+1]\n",
    "        states, rewards, terminated, truncated, info = envs_wrapper.step(actions.cpu().numpy())\n",
    "\n",
    "        ep_action_log_probs[step] = action_log_probs\n",
    "        ep_rewards[step] = torch.tensor(rewards, device=device, dtype=torch.float32)\n",
    "\n",
    "        # mask: if env is ongoing 1 else 0\n",
    "        masks[step] = torch.tensor(~np.logical_or(terminated, truncated))\n",
    "\n",
    "        # reset envs\n",
    "        # get all envs that have completed in all their generations\n",
    "        reset_mask = einx.all(\"(env [gen])\", ~masks[step].to(torch.bool), gen=n_generations)\n",
    "        reset_mask = einx.rearrange(\"env -> (env gen)\", reset_mask, gen=n_generations)\n",
    "        if reset_mask.any():\n",
    "            ep_action_log_probs[:, reset_mask] = 0.\n",
    "            masks[:, reset_mask] = False\n",
    "            envs.reset(seed=step, options=dict(reset_mask=reset_mask.cpu().numpy()))\n",
    "        break\n",
    "\n",
    "    actor_loss = agent.get_losses(\n",
    "        ep_rewards,\n",
    "        ep_action_log_probs,\n",
    "        entropy,\n",
    "        masks,\n",
    "        gamma,\n",
    "        ent_coef,\n",
    "        n_generations,\n",
    "        n_steps_per_update,\n",
    "        device,\n",
    "    )\n",
    "\n",
    "    agent.update_parameters(actor_loss)\n",
    "\n",
    "    stats = {\n",
    "        \"actor_loss\": actor_loss.item(),\n",
    "        \"entropy\": entropy.detach().mean().item()\n",
    "    }\n",
    "    pbar.set_postfix(stats)\n",
    "    actor_losses.append(stats[\"actor_loss\"])\n",
    "    entropies.append(stats[\"entropy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78336d1b-6fa1-4b5f-a418-4ed6150ab906",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" plot the results \"\"\"\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "rolling_length = 20\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 5))\n",
    "fig.suptitle(\n",
    "    f\"Training plots for {agent.__class__.__name__} in the LunarLander-v3 environment \\n \\\n",
    "             (n_envs={n_envs}, n_steps_per_update={n_steps_per_update}, randomize_domain={False})\"\n",
    ")\n",
    "\n",
    "# episode return\n",
    "axs[0][0].set_title(\"Episode Returns\")\n",
    "episode_returns_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(envs_wrapper.return_queue).flatten(),\n",
    "        np.ones(rolling_length),\n",
    "        mode=\"valid\",\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[0][0].plot(\n",
    "    np.arange(len(episode_returns_moving_average)) / n_envs,\n",
    "    episode_returns_moving_average,\n",
    ")\n",
    "axs[0][0].set_xlabel(\"Number of episodes\")\n",
    "\n",
    "# entropy\n",
    "axs[1][0].set_title(\"Entropy\")\n",
    "entropy_moving_average = (\n",
    "    np.convolve(np.array(entropies), np.ones(rolling_length), mode=\"valid\")\n",
    "    / rolling_length\n",
    ")\n",
    "axs[1][0].plot(entropy_moving_average)\n",
    "axs[1][0].set_xlabel(\"Number of updates\")\n",
    "\n",
    "\n",
    "# critic loss\n",
    "axs[0][1].set_title(\"Critic Loss\")\n",
    "critic_losses_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(critic_losses).flatten(), np.ones(rolling_length), mode=\"valid\"\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[0][1].plot(critic_losses_moving_average)\n",
    "axs[0][1].set_xlabel(\"Number of updates\")\n",
    "\n",
    "\n",
    "# actor loss\n",
    "axs[1][1].set_title(\"Actor Loss\")\n",
    "actor_losses_moving_average = (\n",
    "    np.convolve(np.array(actor_losses).flatten(), np.ones(rolling_length), mode=\"valid\")\n",
    "    / rolling_length\n",
    ")\n",
    "axs[1][1].plot(actor_losses_moving_average)\n",
    "axs[1][1].set_xlabel(\"Number of updates\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad328c0-c434-4572-b240-11c186a3d1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_showcase_episodes = 3\n",
    "\n",
    "for episode in range(n_showcase_episodes):\n",
    "    env = gym.make(\"LunarLander-v3\", render_mode=\"human\", max_episode_steps=500)\n",
    "\n",
    "    # get an initial state\n",
    "    state, info = env.reset()\n",
    "\n",
    "    # play one episode\n",
    "    done = False\n",
    "    while not done:\n",
    "        # select an action A_{t} using S_{t} as input for the agent\n",
    "        with torch.no_grad():\n",
    "            action, _, _, _ = agent.select_action(state[None, :])\n",
    "\n",
    "        # perform the action A_{t} in the environment to get S_{t+1} and R_{t+1}\n",
    "        state, reward, terminated, truncated, info = env.step(action.item())\n",
    "\n",
    "        # update if the environment is done\n",
    "        done = terminated or truncated\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af1f3b0-c6b9-4bfd-bc20-a3e011f07ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gymnasium.utils.play import play\n",
    "\n",
    "# play(gym.make('LunarLander-v3', render_mode='rgb_array'),\n",
    "#     keys_to_action={'w': 2, 'a': 1, 'd': 3}, noop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead04b06-05dd-40ab-8794-e18e3b67338d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bf2811-84ea-4f72-b197-04fb5e4827d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
